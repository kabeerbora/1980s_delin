{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kabeerbora/1980s_delin/blob/main/Panel_79_89_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from tqdm.notebook import tqdm\n",
        "import secrets\n",
        "import string\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fklZDmueAZtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Loading and Cleaning\n",
        "\n",
        "### Sampling and Data Collection\n",
        "\n",
        "| **Scheme Code**                 | **Description**                                                                                                                              | **Standard Category** | **Reference** |\n",
        "|--------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------|------------------------|---------------|\n",
        "| **Complete Enumeration**        | Units selected for a complete count, typically large or significant units.                                                                   | Census                 | [ASI 1990-91 Metadata](https://microdata.gov.in/NADA/index.php/catalog/41/study-description) |\n",
        "| **100 or more workers**         | Units employing 100 or more workers, often included in the census sector.                                                                   | Census                 | [ASI 1990-91 Metadata](https://microdata.gov.in/NADA/index.php/catalog/41/study-description) |\n",
        "| **Sample I**                    | A subset of units selected for sampling, possibly based on specific criteria.                                                                | Sample                 | [ASI 1987-88 Documentation](https://microdata.gov.in/nada43/index.php/catalog/38/download/383) |\n",
        "| **Sample II**                   | Another subset of units selected for sampling, possibly based on different criteria than Sample I.                                           | Sample                 | [ASI 1987-88 Documentation](https://microdata.gov.in/nada43/index.php/catalog/38/download/383) |\n",
        "| **B & C 100 or more workers**   | Units in categories B & C with 100 or more workers.                                                                                          | Census                 | [ASI 1987-88 Documentation](https://microdata.gov.in/nada43/index.php/catalog/38/download/383) |\n",
        "| **B & C–CE**                    | B & C category units under Complete Enumeration.                                                                                             | Census                 | [ASI 1987-88 Documentation](https://microdata.gov.in/nada43/index.php/catalog/38/download/383) |\n",
        "| **B & C Sample I**              | B & C category units under Sample I.                                                                                                         | Sample                 | [ASI 1987-88 Documentation](https://microdata.gov.in/nada43/index.php/catalog/38/download/383) |\n",
        "| **B & C Sample II**             | B & C category units under Sample II.                                                                                                        | Sample                 | [ASI 1987-88 Documentation](https://microdata.gov.in/nada43/index.php/catalog/38/download/383) |\n",
        "| **Electricity**                 | Units primarily engaged in electricity generation or distribution.                                                                           | Census                 | [ASI 1990-91 Metadata](https://microdata.gov.in/NADA/index.php/catalog/41/study-description) |\n",
        "| **NR**                          | Not Reported or Not Recorded.                                                                                                                | Exclude                | [ASI 1990-91 Metadata](https://microdata.gov.in/NADA/index.php/catalog/41/study-description) |"
      ],
      "metadata": {
        "id": "3H7Xmdpw7-MT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_file_path = r\"/content/drive/MyDrive/Projects/Annual Survey of India/Data/Copy of 1976_1988_allfirms.csv\"\n",
        "\n",
        "df = pd.read_csv(data_file_path)\n",
        "\n",
        "metric_columns = [\"capital_open\", \"capital_closing\", \"work_cap_open\", \"work_cap_close\", \"outstanding_open\", \"outstanding_close\", \"semi_open\", \"semi_close\"]\n",
        "\n",
        "## 1. Cleaning and generating state code\n",
        "df['state_code'] = df['state_code'].replace(['Daman and Diu'], 'DAMAN  &  DIU')\n",
        "df['state_code'] = df['state_code'].replace(['Dadra & Nagar Haveli'], 'DADRA  AND  NAGAR  HAVELI')\n",
        "\n",
        "unique_state_codes = df['state_code'].unique()\n",
        "state_mapping = {code: i for i, code in enumerate(unique_state_codes)}\n",
        "df['State'] = df['state_code'].map(state_mapping)\n",
        "\n",
        "## 2. Filter census data\n",
        "census_scheme_codes = map(lambda x: x.lower().strip(), [\n",
        "    'Census', 'Complete Enumeration', '100 or more workers',\n",
        "    'B & C 100 or more workers', 'B & C–CE', 'Electricity'\n",
        "])\n",
        "df = df[df['scheme_code'].str.lower().str.strip().isin(census_scheme_codes)]\n",
        "\n",
        "## 3. Remove years\n",
        "years_to_remove = ['1976_1977', '1977_1978','1989_1990', '1990_1991']\n",
        "df = df[~df['year'].isin(years_to_remove)]\n",
        "\n",
        "## 4. Get unique values from 'ownership_code' column\n",
        "df['ownership_code'] = (\n",
        "    df['ownership_code']\n",
        "    .astype(str)\n",
        "    .str.strip()\n",
        "    .str.replace(r'\\s+', ' ', regex=True)\n",
        "    .str.lower()\n",
        ")\n",
        "\n",
        "ownership_code_mapping = {\n",
        "    'wholly private enterprise': 1,\n",
        "    'wholly private ownership': 1,\n",
        "    'wholly central government': 2,\n",
        "    'wholly state and/or local government': 3,\n",
        "    'central government and state and/or local government jointly': 4,\n",
        "    'central government and state and/or local government joint': 4,\n",
        "    'joint sector public': 5,\n",
        "    'joint sector private': 6,\n",
        "    'invalid': 7,\n",
        "    '7.0': 7\n",
        "}\n",
        "\n",
        "df['ownership_code_unique'] = df['ownership_code'].map(ownership_code_mapping)\n",
        "\n",
        "unmatched = df[df['ownership_code_unique'].isna()]['ownership_code'].unique()\n",
        "print(\"Unmatched ownership_code values:\\n\", unmatched)\n",
        "\n",
        "## 5. Generate Identifier\n",
        "df['identifier'] = df['nic_code'].astype(str).str[:4] + '_' + \\\n",
        "                   df['ownership_code_unique'].fillna('nan').astype(str) + '_' + \\\n",
        "                   df['State'].astype(str)\n",
        "\n",
        "df = df.loc[(df['capital_open'] != 0) | (df['capital_closing'] != 0)].copy()\n",
        "\n",
        "## 6. Convert all numerical columns to integer type\n",
        "numeric_cols = df.select_dtypes(include='number').columns\n",
        "df[numeric_cols] = df[numeric_cols].fillna(value=0).astype(int)\n",
        "\n",
        "print(df.shape)\n",
        "df = df.reset_index(drop=True)\n",
        "df = df.reset_index(names=['ROW_ID'])\n",
        "\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "76cPAfGvAo91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Creating Panel\n",
        "\n",
        "Desired Output\n",
        "\n",
        "\n",
        "| firm_Id  | period     | capital         | work           | outstanding      | semi           | year_initial | identifier | source_ROW_IDs       |\n",
        "|----------|------------|------------------|----------------|------------------|----------------|---------------|------------|-----------------------|\n",
        "| ZHY8H7FJ | 1979-1983  | 100,200,300,400  | 250,650,100,150| 500,600,700,800  | 0,1,0,1        | 1957          | 2000_1_3   | 2004,2005,2006,2007   |\n",
        "| AQW9T2KL | 1980-1984  | 150,250,350,450  | 200,300,400,500| 550,650,750,850  | 1,0,1,0        | 1957          | 2000_1_4   | 2010,2011,2012,2013   |\n",
        "| MNB3X9CY | 1981-1985  | 120,220,320,420  | 260,360,460,560| 520,620,720,820  | 0,0,1,1        | 1957          | 2000_1_5   | 2020,2021,2022,2023   |\n",
        "| TYU7P6RE | 1982-1986  | 130,230,330,430  | 270,370,470,570| 530,630,730,830  | 1,1,0,0        | 1957          | 2000_1_6   | 2030,2031,2032,2033   |\n"
      ],
      "metadata": {
        "id": "F36yykzy9iCE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Extract all the years"
      ],
      "metadata": {
        "id": "qxksw7CjJPeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "years = sorted(df['year'].unique())\n",
        "print(years)"
      ],
      "metadata": {
        "id": "72XvQR_PB5se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Helper Functions"
      ],
      "metadata": {
        "id": "slhTC-cUJTaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def standardise_dataframes(df, py_year, cy_year):\n",
        "\n",
        "    ## 1. Extract data\n",
        "    PY_df = df[df['year']==py_year].copy()\n",
        "    CY_df = df[df['year']==cy_year].copy()\n",
        "\n",
        "    ## 2. Standardize Current Year Dataframe\n",
        "    CY_drop_columns = ['capital_closing', 'work_cap_close', 'outstanding_close', 'semi_close']\n",
        "    CY_df = CY_df.drop(CY_drop_columns, axis=1)\n",
        "\n",
        "    CY_rename_dict = {\n",
        "        'capital_open': 'capital',\n",
        "        'work_cap_open': 'work_cap',\n",
        "        'outstanding_open': 'outstanding',\n",
        "        'semi_open': 'semi'\n",
        "    }\n",
        "    CY_df = CY_df.rename(columns = CY_rename_dict)\n",
        "    CY_df = CY_df.drop_duplicates(subset = [\"year_initial\", \"capital\", \"work_cap\", \"outstanding\", \"semi\"])\n",
        "\n",
        "    ## 3. Standardize Previous Year Dataframe\n",
        "    PY_drop_columns = ['capital_open', 'work_cap_open', 'outstanding_open', 'semi_open']\n",
        "    PY_df = PY_df.drop(PY_drop_columns, axis=1)\n",
        "    PY_rename_dict = {\n",
        "        'capital_closing': 'capital',\n",
        "        'work_cap_close': 'work_cap',\n",
        "        'outstanding_close': 'outstanding',\n",
        "        'semi_close': 'semi'\n",
        "    }\n",
        "\n",
        "    PY_df = PY_df.rename(columns = PY_rename_dict)\n",
        "    PY_df = PY_df.drop_duplicates(subset = [\"year_initial\", \"capital\", \"work_cap\", \"outstanding\", \"semi\"])\n",
        "\n",
        "    return CY_df, PY_df"
      ],
      "metadata": {
        "id": "CEKBZ5supx6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Match Row IDs\n",
        "\n",
        "Match row IDS by `opening` and `closing` values along with `identifier`, `year_initial` etc of consecutive years"
      ],
      "metadata": {
        "id": "0gWVEnYwJZgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean = df.copy()\n",
        "columns_to_drop = ['state_code','rsl', 'nic_code', 'ownership_code', 'organization_code', 'scheme_code', 'district_code', 'gross_sales', 'State','ownership_code_unique', 'tot_emoluments', 'bonus_workers', 'wages', 'tot_output', 'value_added', \"persons_engaged\"]\n",
        "df_clean = df_clean.drop(columns_to_drop, axis=1).copy()\n",
        "matched_row_ids = []\n",
        "\n",
        "for i in range(len(years)-1):\n",
        "\n",
        "    ## 1. Extract py and cy years\n",
        "    py_year, cy_year = years[i], years[i+1]\n",
        "\n",
        "    ## 2. Standardize\n",
        "    CY_df, PY_df = standardise_dataframes(df_clean, py_year, cy_year)\n",
        "\n",
        "    ## 3. Merge Dataframes\n",
        "    df_out = pd.merge(\n",
        "        left = CY_df,\n",
        "        right = PY_df,\n",
        "        on = ['year_initial', 'capital', 'work_cap', 'outstanding', 'semi', 'identifier'],\n",
        "        suffixes=('_CY', '_PY'),\n",
        "        how='inner'\n",
        "    )\n",
        "\n",
        "    matched_row_id = list(zip(df_out[\"ROW_ID_PY\"],df_out[\"ROW_ID_CY\"]))\n",
        "    matched_row_ids.append(matched_row_id)\n"
      ],
      "metadata": {
        "id": "6W_SLLKKAu41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Merge Common Row IDS\n",
        "\n",
        "- Now once the common row_ids are found, merge them into one single row\n",
        "- That way we can get the unique firms"
      ],
      "metadata": {
        "id": "Ag57Go_bJ1sD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_row_ids(ROW_IDS_PY, ROW_IDS_CY):\n",
        "\n",
        "    a_dict = dict([(a[:-1], a[-1]) for a in ROW_IDS_PY])\n",
        "    b_dict = dict([(b[:-1], b[-1]) for b in ROW_IDS_CY])\n",
        "    merge_counts = 0\n",
        "\n",
        "    C = []\n",
        "\n",
        "    for a in ROW_IDS_PY:\n",
        "        search_key = (a[-1],)\n",
        "        if search_key in b_dict.keys():\n",
        "            C.append(a+(b_dict[search_key],))\n",
        "            merge_counts +=1\n",
        "        else:\n",
        "            C.append(a)\n",
        "\n",
        "    unused_B = [b for b in ROW_IDS_CY if b[0] not in a_dict.values()]\n",
        "    C.extend(unused_B)\n",
        "\n",
        "    return C, merge_counts"
      ],
      "metadata": {
        "id": "OxL0Rgd4emkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "for i in tqdm(range(len(matched_row_ids))):\n",
        "    print(f\"Iteration: {i}\")\n",
        "    if i == 0:\n",
        "        ROW_IDS_PY = matched_row_ids[i]\n",
        "        ROW_IDS_CY = matched_row_ids[i+1]\n",
        "        PY_row_id = i\n",
        "        CY_row_id = i+1\n",
        "        print(f\"PY matched row ids: {PY_row_id}\")\n",
        "    elif i == 1:\n",
        "        ## Skip this iteration as it was already merged\n",
        "        continue\n",
        "    else:\n",
        "        ROW_IDS_CY = matched_row_ids[i]\n",
        "        CY_row_id = i\n",
        "\n",
        "    ROW_IDS_PY, merge_counts = merge_row_ids(ROW_IDS_PY, ROW_IDS_CY)\n",
        "    print(f\"Total merge counts: {merge_counts}\")\n",
        "\n",
        "print(\"-\"*100)\n",
        "print(f\"Total unique firms found: {len(ROW_IDS_PY)}\")"
      ],
      "metadata": {
        "id": "kGGhEJTaeGI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5 Concat the results"
      ],
      "metadata": {
        "id": "b0nhVRL7JN0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "rows_data = []\n",
        "count = 0\n",
        "for row_ids in tqdm(ROW_IDS_PY, unit=\"rows\", total=len(ROW_IDS_PY)):\n",
        "\n",
        "    x_row_df = df_clean[df_clean[\"ROW_ID\"].isin(row_ids)].copy()\n",
        "\n",
        "    data = {}\n",
        "\n",
        "    ## 1. Extract the metadata values\n",
        "    year_initial, year, identifier = x_row_df.iloc[0][['year_initial', 'year', 'identifier']]\n",
        "    data['year_initial'] = int(year_initial)\n",
        "    data['identifier'] = identifier\n",
        "    data['row_ids'] = row_ids\n",
        "\n",
        "    ## 2. Extract year range\n",
        "    y = x_row_df[['year']].copy()\n",
        "    y[['from', 'to']] = y['year'].str.split(\"_\").tolist()\n",
        "    data['year_from'] = y['from'].iloc[0]\n",
        "    data['year_to'] = y['to'].iloc[-1]\n",
        "\n",
        "    ## 3. Extract all the macro values\n",
        "    data[\"capital\"] = x_row_df[\"capital_open\"].tolist() + [x_row_df[\"capital_closing\"].tolist()[-1]]\n",
        "    data[\"work_cap\"] = x_row_df[\"work_cap_open\"].tolist() + [x_row_df[\"work_cap_close\"].tolist()[-1]]\n",
        "    data[\"outstanding\"] = x_row_df[\"outstanding_open\"].tolist() + [x_row_df[\"outstanding_close\"].tolist()[-1]]\n",
        "    data[\"semi\"] = x_row_df[\"semi_open\"].tolist() + [x_row_df[\"semi_close\"].tolist()[-1]]\n",
        "\n",
        "    rows_data.append(data)\n",
        "\n",
        "df_final = pd.DataFrame(rows_data)"
      ],
      "metadata": {
        "id": "tCBl7hhN8-yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.head()"
      ],
      "metadata": {
        "id": "KWGbM4k8Gdon"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}